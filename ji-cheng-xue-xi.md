##集成学习 ：

>不断通过数据子集 学习 形成新规则然后合并规则

####bagging
1.挑选子集 遵循均匀原则
2.合并规则  平均值

####boosting
1.挑选子集 hardst example
>寻找最难的示例 不去想规则应用于已知分类 而是专注于目前无法分类的示例

2.合并规则  加权平均值
如何加权？
 
####   error 如何定义误差
 错误匹配 mismatches 
 误差率或误差比例
 误差率的定义：
 数据分布d h 代表假设 它是学习器输出的特定假设 c 代表真实的基本概念
 在已知数据分布的情况下在某个样本x上假设和真实概念不一致的概率 

 
![](/assets/F2D5226B-4507-4524-9DB7-4F563BE22A11.png)


####弱学习
 无论数据分布如何该学习算法对 标签的学习 结果都优于随机概率
 无论数据分布如何你所得的误差率都小于1/2
 所有的d 学习算法都会得到预计的误差率 也就是在抽取一个样本时 它与真实概念不同的可能性小于或等于1/2减去 $$\epsilon
$$
 目的 学习器 总能学习一些东西  而随即概率可能性在1/2 你学到的东西为零