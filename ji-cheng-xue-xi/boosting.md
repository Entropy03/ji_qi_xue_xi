##boosting 算法
![](/assets/boosting.png)


### 给定了一个训练集
它由一些 xi, yr 对组成。其中 x 是输入，y 是输出。
标签中会使用 -1 或 +1。其中 -1 表示 不在类别之中，+1 表示 在类别之中。因此这属于二元分类法
####时间步循环
我们将时间步称为小写 t。从时间步 1 一直到将来的更大时间步。我们把循环终止时间步称作大写 T，
 Foo t = 1 to T
 构造分布称为 D(t)（括号中内容为下标）。也就是样本中在特定时间 T 的分布。有了该分布，
####查找弱分类器
弱学习器输出一些假设。我们将结果称为h(t)。也就是在该时间步生成的假设。
该假设应当存在一些小误差。由于误差是一个比较小的数字，我们把它称为 ε(t)。
在培训集上，对于给定的特定分布，
返回的假设称为 h(t)。把有误差的假设称为 ε(t)。也就是说，对于潜在分布，假设与训练标签不一致的可能性非常小。
 ####处理
对整个时间步进行这样的处理，不断寻找每个时间步
存在微小误差 ε(t) 的假设 h(t)。不断构造新的分布。
最后输出最终假设。
AdaBoost算法的具体描述如下：
假定X表示样本空间，Y表示样本类别标识集合，假设是二值分类问题，这里限定Y={-1,+1}。令S={(Xi,yi)|i=1,2,…,m}为样本训练集，其中Xi∈X，yi∈Y。
①:始化m个样本的权值，假设样本分布Dt为均匀分布：Dt(i)=1/m，Dt(i)表示在第t轮迭代中赋给样本(xi,yi)的权值。
②:令T表示迭代的次数。


![](http://img.blog.csdn.net/20131108110611359?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hpdGVpbmJsdWU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
![](http://img.blog.csdn.net/20131108110527796?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2hpdGVpbmJsdWU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
####怎么得到分布
 使用加权后选取的训练数据代替随机选取的训练数据，这样将训练的焦点集中在比较难分的训练数据上。

####怎么获得最终假设
 将弱分类器联合起来时，使用加权的投票机制代替平均投票机制。让分类效果好的弱分类器具有较大的权重，而分类效果差的分类器具有较小的权重。





