##集成学习 ：

>不断通过数据子集 学习 形成新规则然后合并规则

####bagging
1.挑选子集 遵循均匀原则
2.合并规则  平均值

####boosting [文档](http://blog.csdn.net/whiteinblue/article/details/14518773)
```
Boost(推进)，adaboost（adapt boost）自适应推进算法：Adaboost算法是机器学习中一种比较重要的特征分类算法，已被广泛应用人脸表情识别、图像检索等应用中。就目前而言，对Adaboost算法的研究以及应用大多集中于分类问题，在一些回归问题上也有所应用。Adaboost主要解决的问题有: 两类问题、多类单标签问题、多类多标签问题、回归问题。Boost(推进)，adaboost（adapt boost）自适应推进算法：Adaboost算法是机器学习中一种比较重要的特征分类算法，已被广泛应用人脸表情识别、图像检索等应用中。就目前而言，对Adaboost算法的研究以及应用大多集中于分类问题，在一些回归问题上也有所应用。Adaboost主要解决的问题有: 两类问题、多类单标签问题、多类多标签问题、回归问题。
Boosting分类方法，其过程如下所示：

1）先通过对N个训练数据的学习得到第一个弱分类器h1；
2）将h1分错的数据和其他的新数据一起构成一个新的有N个训练数据的样本，通过对这个样本的学习得到第二个弱分类器h2；
3）将h1和h2都分错了的数据加上其他的新数据构成另一个新的有N个训练数据的样本，通过对这个样本的学习得到第三个弱分类器h3；
4）最终经过提升的强分类器h_final=Majority Vote(h1,h2,h3)。即某个数据被分为哪一类要通过h1,h2,h3的多数表决。

上述Boosting算法，存在两个问题：
①如何调整训练集，使得在训练集上训练弱分类器得以进行。
②如何将训练得到的各个弱分类器联合起来形成强分类器。
针对以上两个问题，AdaBoost算法进行了调整：
①使用加权后选取的训练数据代替随机选取的训练数据，这样将训练的焦点集中在比较难分的训练数据上。

②将弱分类器联合起来时，使用加权的投票机制代替平均投票机制。让分类效果好的弱分类器具有较大的权重，而分类效果差的分类器具有较小的权重。


```
1.挑选子集 hardst example
>寻找最难的示例 不去想规则应用于已知分类 而是专注于目前无法分类的示例

2.合并规则  加权平均值
如何加权？
 
####   error 如何定义误差
 错误匹配 mismatches 
 误差率或误差比例
 误差率的定义：
 数据分布d h 代表假设 它是学习器输出的特定假设 c 代表真实的基本概念
 在已知数据分布的情况下在某个样本x上假设和真实概念不一致的概率 

 
![](/assets/F2D5226B-4507-4524-9DB7-4F563BE22A11.png)


####弱学习
 无论数据分布如何该学习算法对 标签的学习 结果都优于随机概率
 无论数据分布如何你所得的误差率都小于1/2
 所有的d 学习算法都会得到预计的误差率 也就是在抽取一个样本时 它与真实概念不同的可能性小于或等于1/2减去 $$\epsilon
$$
 目的 学习器 总能学习一些东西  而随即概率可能性在1/2 你学到的东西为零